{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot with Local LLM\n",
    "\n",
    "A Retrieval-Augmented Generation (RAG) chatbot that answers questions using only internal company documents.\n",
    "\n",
    "**Features:**\n",
    "- Uses local LLM (Qwen2.5-1.5B-Instruct) - no API keys required\n",
    "- Multilingual support (English & German)\n",
    "- Only answers based on provided documents\n",
    "- Admits when information is not available\n",
    "\n",
    "**Architecture:**\n",
    "1. **Document Processing**: Load and chunk markdown documents\n",
    "2. **Embedding**: Multilingual sentence embeddings for semantic search\n",
    "3. **Retrieval**: Find relevant document chunks based on query similarity\n",
    "4. **Generation**: Use LLM to generate answers grounded in retrieved context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "DOCS_DIR = Path(\"data/docs\")\n",
    "EMBEDDING_MODEL = \"intfloat/multilingual-e5-small\"\n",
    "LLM_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Smaller, faster, good multilingual\n",
    "TOP_K = 3  # Number of chunks to retrieve\n",
    "SIMILARITY_THRESHOLD = 0.3  # Minimum similarity to consider context relevant\n",
    "BM25_WEIGHT = 0.35  # Weight for keyword score in hybrid retrieval\n",
    "INDEX_DIR = Path(\"index\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Loading and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 documents:\n",
      "  - travel_expenses.md\n",
      "  - it_security.md\n",
      "  - vacation_policy.md\n",
      "  - employee_handbook.md\n",
      "  - remote_work_policy.md\n",
      "\n",
      "Created 20 chunks total\n"
     ]
    }
   ],
   "source": [
    "def load_documents(docs_dir: Path) -> list[dict]:\n",
    "    \"\"\"Load all markdown documents from directory.\"\"\"\n",
    "    documents = []\n",
    "    for file_path in docs_dir.glob(\"*.md\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            documents.append({\n",
    "                \"filename\": file_path.name,\n",
    "                \"content\": content\n",
    "            })\n",
    "    return documents\n",
    "\n",
    "def extract_title(content: str, filename: str) -> str:\n",
    "    \"\"\"Extract document title from markdown header or derive from filename.\"\"\"\n",
    "    lines = content.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.startswith(\"# \"):\n",
    "            return line[2:].strip()\n",
    "    return filename.replace(\"_\", \" \").replace(\".md\", \"\").title()\n",
    "\n",
    "def chunk_document(doc: dict) -> list[dict]:\n",
    "    \"\"\"Split document into chunks by sections, including title for better retrieval.\"\"\"\n",
    "    content = doc[\"content\"]\n",
    "    filename = doc[\"filename\"]\n",
    "    title = extract_title(content, filename)\n",
    "    \n",
    "    sections = [s.strip() for s in content.split(\"\\n\\n\") if s.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    for section in sections:\n",
    "        if section.startswith(\"# \"):\n",
    "            continue  # Skip title line\n",
    "        if section:\n",
    "            # Prepend title for better semantic matching\n",
    "            chunk_text = f\"[{title}]\\n{section}\"\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"source\": filename,\n",
    "                \"title\": title\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Load and chunk all documents\n",
    "documents = load_documents(DOCS_DIR)\n",
    "print(f\"Loaded {len(documents)} documents:\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc['filename']}\")\n",
    "\n",
    "# Create chunks\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunk_document(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"\\nCreated {len(all_chunks)} chunks total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '[Travel & Expense Guidelines]\\n**1. Booking Travel**  \\nBusiness trips must be booked through the approved travel portal. Economy class is standard for flights under six hours.',\n",
       "  'source': 'travel_expenses.md',\n",
       "  'title': 'Travel & Expense Guidelines'},\n",
       " {'text': '[Travel & Expense Guidelines]\\n**2. Accommodation**  \\nHotel expenses are reimbursed up to 120 EUR per night, including taxes.',\n",
       "  'source': 'travel_expenses.md',\n",
       "  'title': 'Travel & Expense Guidelines'},\n",
       " {'text': '[Travel & Expense Guidelines]\\n**3. Meals**  \\nPer-diem allowances follow local regulations. Receipts must be uploaded within ten days after the trip.',\n",
       "  'source': 'travel_expenses.md',\n",
       "  'title': 'Travel & Expense Guidelines'},\n",
       " {'text': '[Travel & Expense Guidelines]\\n**4. Reimbursement Process**  \\nExpenses are submitted via the HR portal and typically reimbursed within 14 days.',\n",
       "  'source': 'travel_expenses.md',\n",
       "  'title': 'Travel & Expense Guidelines'},\n",
       " {'text': '[IT Security Guidelines]\\n**1. Password Rules**  \\nPasswords must contain at least 12 characters and be changed every 180 days. Multi-factor authentication is mandatory.',\n",
       "  'source': 'it_security.md',\n",
       "  'title': 'IT Security Guidelines'},\n",
       " {'text': '[IT Security Guidelines]\\n**2. Device Usage**  \\nCompany devices may only be used for business purposes. Lost or stolen devices must be reported immediately.',\n",
       "  'source': 'it_security.md',\n",
       "  'title': 'IT Security Guidelines'},\n",
       " {'text': '[IT Security Guidelines]\\n**3. Data Protection**  \\nConfidential data must not be stored on personal devices or shared via unauthorized channels.',\n",
       "  'source': 'it_security.md',\n",
       "  'title': 'IT Security Guidelines'},\n",
       " {'text': '[IT Security Guidelines]\\n**4. Incident Reporting**  \\nSecurity incidents must be reported to the IT security team within 24 hours.',\n",
       "  'source': 'it_security.md',\n",
       "  'title': 'IT Security Guidelines'},\n",
       " {'text': '[Vacation Policy]\\n**1. Annual Leave Entitlement**  \\nEmployees receive 25 days of paid vacation per year. Leave is accrued monthly during the first year.',\n",
       "  'source': 'vacation_policy.md',\n",
       "  'title': 'Vacation Policy'},\n",
       " {'text': '[Vacation Policy]\\n**2. Request Procedure**  \\nVacation requests must be submitted via the HR portal at least two weeks in advance. Approval is subject to team workload and staffing requirements.',\n",
       "  'source': 'vacation_policy.md',\n",
       "  'title': 'Vacation Policy'},\n",
       " {'text': '[Vacation Policy]\\n**3. Carryover Rules**  \\nUp to 5 unused days may be carried over into the next calendar year and must be used by March 31.',\n",
       "  'source': 'vacation_policy.md',\n",
       "  'title': 'Vacation Policy'},\n",
       " {'text': '[Vacation Policy]\\n**4. Special Leave**  \\nSpecial leave may be granted for personal events such as marriage, relocation, or family emergencies.',\n",
       "  'source': 'vacation_policy.md',\n",
       "  'title': 'Vacation Policy'},\n",
       " {'text': '[Employee Handbook]\\n**1. Working Hours**  \\nRegular working hours are 9:00\u201317:00, Monday to Friday. Flexible hours can be arranged with your manager.',\n",
       "  'source': 'employee_handbook.md',\n",
       "  'title': 'Employee Handbook'},\n",
       " {'text': '[Employee Handbook]\\n**2. Probation Period**  \\nAll new employees complete a probation period of six months. During this time, regular check-ins will be conducted.',\n",
       "  'source': 'employee_handbook.md',\n",
       "  'title': 'Employee Handbook'},\n",
       " {'text': '[Employee Handbook]\\n**3. Company Values**  \\nWe value transparency, responsibility, collaboration, and continuous learning.',\n",
       "  'source': 'employee_handbook.md',\n",
       "  'title': 'Employee Handbook'},\n",
       " {'text': '[Employee Handbook]\\n**4. Contacts**  \\nHR inquiries: hr@schaefer.example  \\nIT support: support@schaefer.example  \\nFacilities: facilities@schaefer.example',\n",
       "  'source': 'employee_handbook.md',\n",
       "  'title': 'Employee Handbook'},\n",
       " {'text': '[Remote Work Policy]\\n**1. Eligibility**  \\nEmployees may work remotely for up to three days per week, subject to manager approval.',\n",
       "  'source': 'remote_work_policy.md',\n",
       "  'title': 'Remote Work Policy'},\n",
       " {'text': '[Remote Work Policy]\\n**2. Equipment**  \\nSchaefer Corporation provides a laptop and optional monitor. Employees are responsible for a stable internet connection.',\n",
       "  'source': 'remote_work_policy.md',\n",
       "  'title': 'Remote Work Policy'},\n",
       " {'text': '[Remote Work Policy]\\n**3. Working From Abroad**  \\nRemote work from abroad is permitted for up to 20 days per year. All data-protection rules remain fully applicable.',\n",
       "  'source': 'remote_work_policy.md',\n",
       "  'title': 'Remote Work Policy'},\n",
       " {'text': '[Remote Work Policy]\\n**4. Availability**  \\nDuring working hours, employees must remain reachable via Slack and email.',\n",
       "  'source': 'remote_work_policy.md',\n",
       "  'title': 'Remote Work Policy'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Model and Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: intfloat/multilingual-e5-small\n",
      "Generating embeddings for document chunks...\n",
      "Created embeddings with shape: (20, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "print(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "def compute_docs_hash(docs_dir: Path) -> str:\n",
    "    hasher = hashlib.sha256()\n",
    "    for file_path in sorted(docs_dir.glob(\"*.md\")):\n",
    "        hasher.update(file_path.name.encode(\"utf-8\"))\n",
    "        hasher.update(b\"\\0\")\n",
    "        hasher.update(file_path.read_bytes())\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def load_cached_index(docs_hash: str):\n",
    "    INDEX_DIR.mkdir(exist_ok=True)\n",
    "    meta_path = INDEX_DIR / \"index_meta.json\"\n",
    "    embed_path = INDEX_DIR / \"embeddings.npy\"\n",
    "    chunks_path = INDEX_DIR / \"chunks.json\"\n",
    "    if not (meta_path.exists() and embed_path.exists() and chunks_path.exists()):\n",
    "        return None\n",
    "    meta = json.loads(meta_path.read_text())\n",
    "    if meta.get(\"docs_hash\") != docs_hash:\n",
    "        return None\n",
    "    if meta.get(\"embedding_model\") != EMBEDDING_MODEL:\n",
    "        return None\n",
    "    cached_chunks = json.loads(chunks_path.read_text())\n",
    "    cached_embeddings = np.load(embed_path)\n",
    "    if len(cached_chunks) != len(cached_embeddings):\n",
    "        return None\n",
    "    return cached_chunks, cached_embeddings\n",
    "\n",
    "def save_cached_index(docs_hash: str, chunks: list[dict], embeddings: np.ndarray) -> None:\n",
    "    INDEX_DIR.mkdir(exist_ok=True)\n",
    "    meta = {\"docs_hash\": docs_hash, \"embedding_model\": EMBEDDING_MODEL}\n",
    "    (INDEX_DIR / \"index_meta.json\").write_text(json.dumps(meta, ensure_ascii=True))\n",
    "    (INDEX_DIR / \"chunks.json\").write_text(json.dumps(chunks, ensure_ascii=True))\n",
    "    np.save(INDEX_DIR / \"embeddings.npy\", embeddings)\n",
    "\n",
    "print(\"Generating embeddings for document chunks...\")\n",
    "docs_hash = compute_docs_hash(DOCS_DIR)\n",
    "cached = load_cached_index(docs_hash)\n",
    "if cached:\n",
    "    all_chunks, chunk_embeddings = cached\n",
    "    print(f\"Loaded cached embeddings: {chunk_embeddings.shape}\")\n",
    "else:\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "    chunk_embeddings = embed_model.encode([f\"passage: {t}\" for t in chunk_texts], normalize_embeddings=True)\n",
    "    save_cached_index(docs_hash, all_chunks, chunk_embeddings)\n",
    "    print(f\"Created embeddings with shape: {chunk_embeddings.shape}\")\n",
    "\n",
    "# Ensure chunk_texts is available for BM25\n",
    "chunk_texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "# Build BM25 index for keyword retrieval\n",
    "def bm25_tokenize(text: str) -> list[str]:\n",
    "    return [t for t in \"\".join(ch if ch.isalnum() or ch.isspace() else \" \" for ch in text.lower()).split() if t]\n",
    "\n",
    "bm25_corpus = [bm25_tokenize(t) for t in chunk_texts]\n",
    "bm25 = BM25Okapi(bm25_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many vacation days do I get?\n",
      "\n",
      "1. [vacation_policy.md] (score: 0.892)\n",
      "   [Vacation Policy]\n",
      "**1. Annual Leave Entitlement**  \n",
      "Employees receive 25 days of paid vacation per y...\n",
      "\n",
      "2. [vacation_policy.md] (score: 0.866)\n",
      "   [Vacation Policy]\n",
      "**3. Carryover Rules**  \n",
      "Up to 5 unused days may be carried over into the next cal...\n",
      "\n",
      "3. [vacation_policy.md] (score: 0.861)\n",
      "   [Vacation Policy]\n",
      "**2. Request Procedure**  \n",
      "Vacation requests must be submitted via the HR portal a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def expand_query(query: str) -> str:\n",
    "    \"\"\"Add lightweight synonym hints for better recall.\"\"\"\n",
    "    q = query.strip()\n",
    "    q_lower = q.lower()\n",
    "    additions = []\n",
    "    # English synonyms\n",
    "    if \"work from home\" in q_lower or \"wfh\" in q_lower:\n",
    "        additions += [\"remote work\", \"home office\", \"telework\"]\n",
    "    if \"remote work\" in q_lower:\n",
    "        additions += [\"work from home\", \"home office\"]\n",
    "    # German synonyms\n",
    "    if \"von zu hause\" in q_lower or \"zu hause\" in q_lower:\n",
    "        additions += [\"remote arbeiten\", \"homeoffice\", \"telearbeit\"]\n",
    "    if \"homeoffice\" in q_lower:\n",
    "        additions += [\"remote arbeiten\", \"von zu hause arbeiten\"]\n",
    "\n",
    "    if not additions:\n",
    "        return q\n",
    "    # Append synonyms to help embedding recall\n",
    "    return q + \" | \" + \" | \".join(sorted(set(additions)))\n",
    "\n",
    "\n",
    "def semantic_search(query: str, top_k: int = TOP_K) -> list[dict]:\n",
    "    \"\"\"Find most relevant chunks for a query using cosine similarity.\"\"\"\n",
    "    # Embed the query\n",
    "    expanded_query = expand_query(query)\n",
    "    query_embedding = embed_model.encode([f\"query: {expanded_query}\"], normalize_embeddings=True)[0]\n",
    "    \n",
    "    # Calculate cosine similarities (embeddings are normalized, so dot product = cosine)\n",
    "    similarities = np.dot(chunk_embeddings, query_embedding)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # Return results with scores\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"text\": all_chunks[idx][\"text\"],\n",
    "            \"source\": all_chunks[idx][\"source\"],\n",
    "            \"score\": float(similarities[idx])\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test semantic search\n",
    "test_query = \"How many vacation days do I get?\"\n",
    "results = semantic_search(test_query)\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [{r['source']}] (score: {r['score']:.3f})\")\n",
    "    print(f\"   {r['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading LLM: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Determine best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loading LLM: {LLM_MODEL}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL,\n",
    "    dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful company assistant that answers questions based ONLY on the provided context.\n",
    "\n",
    "STRICT RULES:\n",
    "1. ONLY use information from the provided context to answer questions\n",
    "2. If the context does not contain the answer, say: \"I don't have information about that in our company documents.\"\n",
    "3. Never make up or infer information not explicitly stated in the context\n",
    "4. Respond in the same language as the user's question (German or English)\n",
    "5. Keep answers concise but complete\n",
    "6. If asked about topics outside the context (weather, general knowledge, etc.), politely explain you can only answer questions about company policies\"\"\"\n",
    "\n",
    "\n",
    "def build_context(search_results: list[dict], threshold: float = SIMILARITY_THRESHOLD) -> tuple[str, bool]:\n",
    "    \"\"\"Build context string from search results. Returns (context, has_relevant_info).\"\"\"\n",
    "    # Filter by similarity threshold\n",
    "    relevant_results = [r for r in search_results if r[\"score\"] >= threshold]\n",
    "    \n",
    "    if not relevant_results:\n",
    "        return \"\", False\n",
    "    \n",
    "    context_parts = []\n",
    "    for r in relevant_results:\n",
    "        context_parts.append(f\"[From {r['source']}]:\\n{r['text']}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts), True\n",
    "\n",
    "\n",
    "def generate_response(query: str, context: str, has_context: bool | None = None) -> str:\n",
    "    \"\"\"Generate response using the LLM with proper chat template.\"\"\"\n",
    "    \n",
    "    if has_context:\n",
    "        user_message = f\"\"\"Context from company documents:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based ONLY on the context above:\"\"\"\n",
    "    else:\n",
    "        user_message = f\"\"\"Question: {query}\n",
    "\n",
    "Note: No relevant information was found in the company documents for this question.\"\"\"\n",
    "    \n",
    "    # Build messages for chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def ask(query: str) -> str:\n",
    "    \"\"\"Main RAG function: retrieve context and generate answer.\"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    search_results = semantic_search(query, top_k=TOP_K)\n",
    "    \n",
    "    # Step 2: Build context with relevance check\n",
    "    context, has_relevant_context = build_context(search_results)\n",
    "    \n",
    "    # Step 3: Generate response\n",
    "    response = generate_response(query, context, has_relevant_context)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENGLISH QUERIES\n",
      "============================================================\n",
      "\n",
      "Q: How many vacation days do employees get per year?\n",
      "A: Employees receive 25 days of paid vacation per year.\n",
      "----------------------------------------\n",
      "\n",
      "Q: What are the password requirements?\n",
      "A: According to the IT Security Guidelines, passwords must contain at least 12 characters and need to be changed every 180 days. Multi-factor authentication is also mandatory for accessing systems.\n",
      "----------------------------------------\n",
      "\n",
      "Q: Can I work from home?\n",
      "A: Yes, you can work from home if it meets the following conditions:\n",
      "\n",
      "- You are eligible under the Remote Work Policy with your manager's approval.\n",
      "- Your request should be submitted at least one month before the intended start date.\n",
      "- The period of remote work cannot exceed 20 days per year.\n",
      "- During working hours, you must remain reachable via Slack and email.\n",
      "----------------------------------------\n",
      "\n",
      "Q: Do I need my own laptop?\n",
      "A: Yes, Schaefer Corporation provides a laptop to employees, so you do need your own laptop.\n",
      "----------------------------------------\n",
      "\n",
      "Q: What is the weather like today?\n",
      "A: I don't have information about that in our company documents.\n",
      "----------------------------------------\n",
      "\n",
      "Q: What is the capital of France?\n",
      "A: I don't have information about that in our company documents.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with English questions\n",
    "test_questions_en = [\n",
    "    \"How many vacation days do employees get per year?\",\n",
    "    \"What are the password requirements?\",\n",
    "    \"Can I work from home?\",\n",
    "    \"Do I need my own laptop?\",\n",
    "    \"What is the weather like today?\",  # Should say \"I don't know\"\n",
    "    \"What is the capital of France?\"  # Should say \"I don't know\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENGLISH QUERIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in test_questions_en:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    answer = ask(q)\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GERMAN QUERIES\n",
      "============================================================\n",
      "\n",
      "Q: Wie viele Urlaubstage habe ich pro Jahr?\n",
      "A: Ich habe pro Jahr 25 Tage freien Urlaub.\n",
      "----------------------------------------\n",
      "\n",
      "Q: Welche Passwortanforderungen gibt es?\n",
      "A: Passw\u00f6rter m\u00fcssen mindestens 12 Zeichen lang sein und jedes Jahr umgelegt werden. Mehrfachauthentifizierung ist erforderlich.\n",
      "----------------------------------------\n",
      "\n",
      "Q: Kann ich von zu Hause aus arbeiten?\n",
      "A: Ja, Sie k\u00f6nnen von zu Hause aus arbeiten, aber nur f\u00fcr bis zu 20 Tage pro Jahr. Alle Datenschutzregeln bleiben vollst\u00e4ndig anwendbar.\n",
      "----------------------------------------\n",
      "\n",
      "Q: Wie ist das Wetter heute?\n",
      "A: Ich kann Ihnen leider nicht sagen, wie es heute im Wetter ist, da ich keine Informationen dar\u00fcber habe.\n",
      "----------------------------------------\n",
      "\n",
      "Q: Was ist die Hauptstadt von Frankreich?\n",
      "A: Ich kann Ihnen leider keine Informationen \u00fcber die Hauptstadt Frankreichs geben, da sie nicht im angegebenen Kontext erw\u00e4hnt wird.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with German questions\n",
    "test_questions_de = [\n",
    "    \"Wie viele Urlaubstage habe ich pro Jahr?\",\n",
    "    \"Welche Passwortanforderungen gibt es?\",\n",
    "    \"Kann ich von zu Hause aus arbeiten?\",\n",
    "    \"Wie ist das Wetter heute?\",  # Should say \"I don't know\"\n",
    "    \"Was ist die Hauptstadt von Frankreich?\",  # Should say \"I don't know\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GERMAN QUERIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in test_questions_de:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    answer = ask(q)\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Debug View (Optional)\n",
    "\n",
    "Use this to see the retrieval results and understand how the system works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the hotel limit for business travel?\n",
      "\n",
      "Retrieved 3 chunks:\n",
      "  1. [travel_expenses.md] score=0.401\n",
      "  2. [travel_expenses.md] score=0.262\n",
      "  3. [travel_expenses.md] score=0.216\n",
      "\n",
      "Has relevant context: True\n",
      "\n",
      "Response: I don't have information about that in our company documents.\n"
     ]
    }
   ],
   "source": [
    "def ask_debug(query: str) -> dict:\n",
    "    \"\"\"RAG with debug info showing retrieval results.\"\"\"\n",
    "    # Step 1: Retrieve\n",
    "    search_results = semantic_search(query, top_k=TOP_K)\n",
    "    \n",
    "    # Step 2: Build context\n",
    "    context, has_relevant_context = build_context(search_results)\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    response = generate_response(query, context, has_relevant_context)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_chunks\": search_results,\n",
    "        \"has_relevant_context\": has_relevant_context,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "# Example debug view\n",
    "debug_result = ask_debug(\"What is the hotel limit for business travel?\")\n",
    "\n",
    "print(f\"Query: {debug_result['query']}\")\n",
    "print(f\"\\nRetrieved {len(debug_result['retrieved_chunks'])} chunks:\")\n",
    "for i, chunk in enumerate(debug_result['retrieved_chunks'], 1):\n",
    "    print(f\"  {i}. [{chunk['source']}] score={chunk['score']:.3f}\")\n",
    "print(f\"\\nHas relevant context: {debug_result['has_relevant_context']}\")\n",
    "print(f\"\\nResponse: {debug_result['response']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}