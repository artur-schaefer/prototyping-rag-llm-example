{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3469ffe-623d-48ca-bb8f-f81741c4e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langdetect import detect, DetectorFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d39e36a2-0065-4bd9-a1e9-deacdeab1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectorFactory.seed = 0 #fix langdetect randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a248c0-69a4-413a-acc6-3eeb77cbbcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/arturschaefer/Documents/Freelancing/Repos/rag-llm-example/data/docs'),\n",
       " PosixPath('/Users/arturschaefer/Documents/Freelancing/Repos/rag-llm-example/index'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base paths\n",
    "BASE_DIR = Path().resolve()\n",
    "DOCS_DIR = BASE_DIR / \"data\" / \"docs\"\n",
    "INDEX_DIR = BASE_DIR / \"index\"\n",
    "\n",
    "DOCS_DIR, INDEX_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ec9ae2-0f19-4729-9f74-4bfa06f0093a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " ['employee_handbook',\n",
       "  'it_security',\n",
       "  'remote_work_policy',\n",
       "  'travel_expenses',\n",
       "  'vacation_policy'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load markdown documents\n",
    "\n",
    "def load_documents(docs_dir: Path) -> List[Dict]:\n",
    "    documents = []\n",
    "    for path in sorted(docs_dir.glob(\"*.md\")):\n",
    "        text = path.read_text(encoding=\"utf-8\")\n",
    "        lines = text.splitlines()\n",
    "        \n",
    "        # use first Markdown heading as title\n",
    "        if lines and lines[0].lstrip().startswith(\"#\"):\n",
    "            title = lines[0].lstrip(\"#\").strip()\n",
    "        else:\n",
    "            title = path.stem\n",
    "        \n",
    "        documents.append({\n",
    "            \"id\": path.stem,\n",
    "            \"title\": title,\n",
    "            \"text\": text\n",
    "        })\n",
    "    return documents\n",
    "\n",
    "docs = load_documents(DOCS_DIR)\n",
    "len(docs), [d[\"id\"] for d in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518aafd4-8a74-4ed4-8930-457a5f2570d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunk documents (simple double-newline split)\n",
    "\n",
    "def chunk_document(doc: Dict, min_chars: int = 40) -> List[Dict]:\n",
    "    raw_chunks = [c.strip() for c in doc[\"text\"].split(\"\\n\\n\") if c.strip()]\n",
    "    chunks = []\n",
    "\n",
    "    for i, c in enumerate(raw_chunks):\n",
    "        if len(c) < min_chars:\n",
    "            continue\n",
    "        chunks.append({\n",
    "            \"chunk_id\": f\"{doc['id']}-{i}\",\n",
    "            \"doc_id\": doc[\"id\"],\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"text\": c,\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "all_chunks = []\n",
    "for d in docs:\n",
    "    all_chunks.extend(chunk_document(d))\n",
    "\n",
    "len(all_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eaae663-22a1-4c68-ae6e-87b44ca50618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
    "embed_model = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "texts = [c[\"text\"] for c in all_chunks]\n",
    "embeddings = embed_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c07f035-be26-4ba2-bfba-a1fe23ca18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "\n",
    "np.save(INDEX_DIR / \"embeddings.npy\", embeddings)\n",
    "with open(INDEX_DIR / \"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5675cddb-c95d-431e-8254-eab1edee4418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk_id': 'vacation_policy-1',\n",
       "  'doc_id': 'vacation_policy',\n",
       "  'title': 'Vacation Policy',\n",
       "  'text': '**1. Annual Leave Entitlement**  \\nEmployees receive 25 days of paid vacation per year. Leave is accrued monthly during the first year.',\n",
       "  'score': 0.31855595111846924},\n",
       " {'chunk_id': 'vacation_policy-2',\n",
       "  'doc_id': 'vacation_policy',\n",
       "  'title': 'Vacation Policy',\n",
       "  'text': '**2. Request Procedure**  \\nVacation requests must be submitted via the HR portal at least two weeks in advance. Approval is subject to team workload and staffing requirements.',\n",
       "  'score': 0.259296715259552},\n",
       " {'chunk_id': 'remote_work_policy-1',\n",
       "  'doc_id': 'remote_work_policy',\n",
       "  'title': 'Remote Work Policy',\n",
       "  'text': '**1. Eligibility**  \\nEmployees may work remotely up to three days per week, subject to manager approval.',\n",
       "  'score': 0.2439272403717041}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever (semantic search)\n",
    "\n",
    "def retrieve(query: str, k: int = 5) -> List[Dict]:\n",
    "    q_emb = embed_model.encode([query], normalize_embeddings=True)[0]\n",
    "    sims = embeddings @ q_emb  # cosine similarity since normalized\n",
    "    top_idx = np.argsort(-sims)[:k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        c = all_chunks[idx]\n",
    "        results.append({\n",
    "            \"chunk_id\": c[\"chunk_id\"],\n",
    "            \"doc_id\": c[\"doc_id\"],\n",
    "            \"title\": c[\"title\"],\n",
    "            \"text\": c[\"text\"],\n",
    "            \"score\": float(sims[idx]),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "retrieve(\"Wie viele Urlaubstage habe ich?\", k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c44468-c49c-420e-b847-602bcc9a95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language detection\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    cleaned = text.strip()\n",
    "    if not cleaned:\n",
    "        return \"en\"\n",
    "     \n",
    "    lang = detect(cleaned)\n",
    "    if lang in [\"de\", \"en\"]:\n",
    "        return lang\n",
    "    else: return \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fb513-3b41-4991-8f87-a67e4b52524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the rag prompt - optimized for flan-t5\n",
    "\n",
    "def build_prompt(query: str, contexts: List[Dict]) -> str:\n",
    "    user_lang = detect_language(query)\n",
    "\n",
    "    # keep context short â€“ flan-t5-base has limited context\n",
    "    context_parts = []\n",
    "    for c in contexts:\n",
    "        context_parts.append(c[\"text\"])\n",
    "    context_str = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    if user_lang == \"de\":\n",
    "        # Simplified instruction that works better with flan-t5\n",
    "        instruction = (\n",
    "            \"Answer the question based on the context below. Answer in full sentences. \"\n",
    "            \"Answer in German.\\n\\n\"\n",
    "        )\n",
    "        prompt = (\n",
    "            instruction +\n",
    "            \"Context:\\n\" + context_str + \"\\n\\n\" +\n",
    "            \"Frage: \" + query + \"\\n\" +\n",
    "            \"Antwort:\"\n",
    "        )\n",
    "    else:\n",
    "        instruction = (\n",
    "            \"Answer the question based on the context below. Answer in full sentences.\\n\\n\"\n",
    "        )\n",
    "        prompt = (\n",
    "            instruction +\n",
    "            \"Context:\\n\" + context_str + \"\\n\\n\" +\n",
    "            \"Question: \" + query + \"\\n\" +\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63f749bc-b4f4-4bf0-935f-a870a55af13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c94ff58b094ca9b60b2694b4aa074a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "# local llm using transformers - Qwen model\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "577bef92-317a-4dd8-94c7-2002bec6ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm call helper - optimized for better context usage\n",
    "\n",
    "def call_local_llm(messages: List[Dict], max_new_tokens: int = 200) -> str:\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with parameters encouraging fuller responses\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=0.1,  # Slightly higher for more creative synthesis\n",
    "        repetition_penalty=1.15,\n",
    "        no_repeat_ngram_size=3  # Prevent repetitive phrases\n",
    "    )\n",
    "    \n",
    "    # Decode only the new tokens (exclude input)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] \n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7a4b0fd-016c-4ef0-8a60-972478284aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Qwen2.5-3B-Instruct is a strong multilingual model\n",
    "# It handles German much better than FLAN-T5\n",
    "# Other good alternatives:\n",
    "# - \"Qwen/Qwen2.5-7B-Instruct\" (larger, better quality)\n",
    "# - \"meta-llama/Llama-3.2-3B-Instruct\" (similar size, good multilingual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51768962-1c1d-46ac-9bce-c7378df64a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full rag pipeline - optimized for comprehensive answers\n",
    "\n",
    "def rag_answer(query: str, k: int = 3) -> Dict:\n",
    "    # Retrieve relevant contexts\n",
    "    contexts = retrieve(query, k=k)\n",
    "    \n",
    "    # Build chat messages with numbered sources\n",
    "    messages = build_prompt(query, contexts)\n",
    "    \n",
    "    # Generate answer with more tokens for complete responses\n",
    "    answer = call_local_llm(messages, max_new_tokens=200)\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": contexts,\n",
    "        \"num_sources\": len(contexts)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ebf97d0-d871-4ba2-999c-728af83de825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to see what context the model receives\n",
    "\n",
    "def preview_prompt(query: str, k: int = 3):\n",
    "    contexts = retrieve(query, k=k)\n",
    "    messages = build_prompt(query, contexts)\n",
    "    \n",
    "    print(\"=== SYSTEM MESSAGE ===\")\n",
    "    print(messages[0][\"content\"])\n",
    "    print(\"\\n=== USER MESSAGE (first 800 chars) ===\")\n",
    "    print(messages[1][\"content\"][:800])\n",
    "    print(\"...\")\n",
    "    print(f\"\\n[Total context length: {len(messages[1]['content'])} characters]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c75db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81c11e91-35c7-42dc-bbb4-8ea0fa625a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort:\n",
      "Yes, I'm Qwen. How can I assist you today? Whether it's answering questions, providing information on various topics, or engaging in conversation, feel free to ask me anything!\n",
      "\n",
      "\n",
      "Quellen:\n",
      "- Vacation Policy: **1. Annual Leave Entitlement**  \n",
      "Employees receive 25 days of paid vacation per year. Leave is accr... (score: 0.304)\n",
      "- Vacation Policy: **2. Request Procedure**  \n",
      "Vacation requests must be submitted via the HR portal at least two weeks ... (score: 0.215)\n",
      "- Travel & Expense Guidelines: **3. Meals**  \n",
      "Per-diem allowances follow local regulations. Receipts must be uploaded within ten da... (score: 0.209)\n"
     ]
    }
   ],
   "source": [
    "res = rag_answer(\"How much vacation do I get?\", k=3)\n",
    "\n",
    "print(\"Antwort:\")\n",
    "print(res[\"answer\"])\n",
    "\n",
    "print(\"\\n\\nQuellen:\")\n",
    "for s in res[\"sources\"]:\n",
    "    print(f\"- {s['title']}: {s['text'][:100]}... (score: {s['score']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a0d4f3-8ed5-45e7-a146-1fcc9ca252d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb2080-1686-46dc-a089-956f2b4d6986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698f81f-3669-4610-9f7b-f706063fddfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289fe2a-9128-4c9c-b0ed-3392c39e0244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
